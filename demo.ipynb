{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import List, Type\n",
    "from os import environ\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Exam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m responses\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#this is still full of dummy data (but it runs!)\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mLLMTest\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_llm\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mta_llm\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexam\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mExam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudent_llm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstudent_llm\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 128\u001b[0m, in \u001b[0;36mLLMTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLMTest\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, student_llm: Llm, ta_llm: Llm, exam: \u001b[43mExam\u001b[49m):\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent_llm \u001b[38;5;241m=\u001b[39m student_llm\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mta_llm \u001b[38;5;241m=\u001b[39m ta_llm\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Exam' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GradeLog:\n",
    "    TA: str\n",
    "    Student: str\n",
    "    Prompt: str\n",
    "    Response: str\n",
    "    Notes: str\n",
    "    Grade: int\n",
    "\n",
    "class Exam:\n",
    "    def __init__(self, questions: List[str], question_guidelines: List[str], exam_guidelines: List[str], schema: GradeLog):\n",
    "        assert issubclass(schema, GradeLog), \"Provided schema must be a subclass of Schema or Schema itself.\"\n",
    "\n",
    "        self.questions = questions\n",
    "        self.question_guidelines = question_guidelines\n",
    "        self.exam_guidelines = exam_guidelines\n",
    "        self.schema = GradeLog\n",
    "    \n",
    "    def summarize_exam(self):\n",
    "        print(\"Exam Guidelines:\")\n",
    "        for guideline in self.exam_guidelines:\n",
    "            print(f\"\\t{guideline}\")\n",
    "        print(\"Questions:\")\n",
    "        for i, question in enumerate(self.questions):\n",
    "            print(f\"\\t{i+1}. {question}\")\n",
    "            print(f\"\\t\\t{self.question_guidelines[i]}\")\n",
    "\n",
    "    # Additional methods can be added here\n",
    "\n",
    "\n",
    "class Llm:\n",
    "    def __init__(self, model_identifier: str = \"gpt-4-1106-preview\", \n",
    "                 url: str = \"https://api.openai.com/v1/chat/completions\", \n",
    "                 role: str = \"user\",\n",
    "                 auth: dict = {\"Authorization\": f\"Bearer {environ.get('OPENAI_API_KEY')}\"}):\n",
    "        \n",
    "        self.model_identifier = model_identifier\n",
    "        self.url = url\n",
    "        self.role = role\n",
    "        self.auth = auth\n",
    "\n",
    "    #add setters and getters for the above attributes\n",
    "    # Getter for model_identifier\n",
    "    @property\n",
    "    def model_identifier(self):\n",
    "        return self._model_identifier\n",
    "\n",
    "    # Setter for model_identifier\n",
    "    @model_identifier.setter\n",
    "    def model_identifier(self, value):\n",
    "        self._model_identifier = value\n",
    "\n",
    "    # Getter for url\n",
    "    @property\n",
    "    def url(self):\n",
    "        return self._url\n",
    "\n",
    "    # Setter for url\n",
    "    @url.setter\n",
    "    def url(self, value):\n",
    "        self._url = value\n",
    "\n",
    "    # Getter for role\n",
    "    @property\n",
    "    def role(self):\n",
    "        return self._role\n",
    "\n",
    "    # Setter for role\n",
    "    @role.setter\n",
    "    def role(self, value):\n",
    "        self._role = value\n",
    "\n",
    "    # Getter for auth\n",
    "    @property\n",
    "    def auth(self):\n",
    "        return self._auth\n",
    "\n",
    "    # Setter for auth\n",
    "    @auth.setter\n",
    "    def auth(self, value):\n",
    "        self._auth = value\n",
    "        \n",
    "    def prompt(self, text: str) -> str:\n",
    "        # Method to send a prompt to the LLM and return its response\n",
    "        url = self.url\n",
    "        req = {\n",
    "            \"model\": self.model_identifier,\n",
    "            \"messages\":[\n",
    "                {\"role\": self.role, \"content\": text}\n",
    "            ]\n",
    "        }\n",
    "        print(req)\n",
    "        response = requests.post(url, json=req, headers=self.auth)  # Use json parameter to send the request payload as JSON\n",
    "        raw =  response.json()\n",
    "        try:\n",
    "            return f\"{response.json()['choices'][0]['message']['content']}\"\n",
    "        except:\n",
    "            return raw\n",
    "        \n",
    "    # def prompt_sequence(self, prompts: List[str]) -> List[str]:\n",
    "    #     \"\"\"\n",
    "    #     Method to send a sequence of prompts to the LLM and return its response.\n",
    "    #     Each prompt in the sequence is sent as a separate message in a single request.\n",
    "    #     \"\"\"\n",
    "    #     url = self.url\n",
    "    #     messages = [{\"role\": self.role, \"content\": prompt} for prompt in prompts]\n",
    "        \n",
    "    #     req = {\n",
    "    #         \"model\": self.model_identifier,\n",
    "    #         \"messages\": messages\n",
    "    #     }\n",
    "    #     print(req)\n",
    "    #     response = requests.post(url, json=req, headers=self.auth)  # Use json parameter to send the request payload as JSON\n",
    "    #     raw = response.json()\n",
    "    #     print(raw)\n",
    "    #     try:\n",
    "    #         return [resp['message']['content'] for resp in raw['choices']]\n",
    "    #     except:\n",
    "    #         return raw\n",
    "        \n",
    "    def prompt_sequence(self, prompts: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Method to send a sequence of prompts to the LLM and return its responses.\n",
    "        Each prompt is sent in a separate request, maintaining the conversation history.\n",
    "        \"\"\"\n",
    "        conversation_history = []\n",
    "        responses = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            # Concatenate all previous elements of the conversation for context\n",
    "            full_prompt = \" \".join(conversation_history + [prompt])\n",
    "\n",
    "            # Create the request payload\n",
    "            req = {\n",
    "                \"model\": self.model_identifier,\n",
    "                \"messages\": [{\"role\": self.role, \"content\": full_prompt}]\n",
    "            }\n",
    "\n",
    "            response = requests.post(self.url, json=req, headers=self.auth)\n",
    "            raw = response.json()\n",
    "\n",
    "            try:\n",
    "                # Extract the response content\n",
    "                content = raw['choices'][0]['message']['content']\n",
    "                responses.append(content)\n",
    "                # Update the conversation history\n",
    "                conversation_history.append(prompt)\n",
    "                conversation_history.append(content)\n",
    "            except:\n",
    "                # In case of an error, append the raw response for debugging\n",
    "                responses.append(raw)\n",
    "\n",
    "        return responses\n",
    "\n",
    "\n",
    "#this is still full of dummy data (but it runs!)\n",
    "class LLMTest:\n",
    "    def __init__(self, student_llm: Llm, ta_llm: Llm, exam: Exam):\n",
    "        self.student_llm = student_llm\n",
    "        self.ta_llm = ta_llm\n",
    "        self.exam = exam\n",
    "\n",
    "    def test(self) -> List[Type[GradeLog]]:\n",
    "        student_responses = self.student_llm.prompt_sequence(self.exam.questions)\n",
    "        graded_responses = self.ta_llm.prompt_sequence(self.format_grading_prompt_sequence(student_responses))\n",
    "        standardized_responses = [self.process_ta_response(self.exam.questions[i], student_responses[i], graded_responses[i+1]) for i in range(len(self.exam.questions))]\n",
    "\n",
    "        return standardized_responses\n",
    "\n",
    "    \n",
    "    def format_grading_prompt_sequence(self, responses: str) -> str:\n",
    "        assert(len(self.exam.questions) == len(responses))\n",
    "        # Format the grading prompt for the TA LLM\n",
    "        context_str = \"\"\"You are grading an exam. For each of the follow question:response pairs please provide a grade and notes for the student.\n",
    "         the grade and the notes for every question should evaluated according to the following guidelines:\"\"\"\n",
    "        context_str += \"\\n\".join(self.exam.exam_guidelines)+\"\\n As the TA, the professor will be evaluating your evaluations as part of your teaching practicum; your PhD candidacy depends on this. Are you ready to begin?\"\n",
    "\n",
    "        def format_grading_prompt(question: str, response: str, guideline: str) -> str:\n",
    "            prompt_str = \"Question: \"+question+\"\\n received the following response: \"+response+\"\\n Please provide a grade and notes for the student, according to the following guidelines: \"+guideline\n",
    "\n",
    "            return prompt_str\n",
    "        \n",
    "        sequence = [context_str]+[format_grading_prompt(self.exam.questions[i], responses[i], self.exam.question_guidelines[i]) for i in range(len(self.exam.questions))]\n",
    "        return sequence   \n",
    "       \n",
    "\n",
    "    def process_ta_response(self, question, student_response, ta_response: str) -> Type[GradeLog]:\n",
    "        # Process the TA's response and return it in the schema format\n",
    "        # Placeholder implementation; this should be tailored to parse the actual TA's response\n",
    "        def process_string(input_str: str):\n",
    "            # Regex pattern to find 'Grade' followed by an integer\n",
    "            pattern = r\"Grade: (\\d+)\"\n",
    "\n",
    "            # Search for the pattern in the input string\n",
    "            match = re.search(pattern, input_str)\n",
    "\n",
    "            if match:\n",
    "                # Extract the grade\n",
    "                grade = int(match.group(1))\n",
    "\n",
    "                # Cut the 'Grade' portion out of the string\n",
    "                notes = input_str.replace(match.group(0), '').strip()\n",
    "\n",
    "                return notes, grade\n",
    "            else:\n",
    "                # Return the original string and a default grade if 'Grade' not found\n",
    "                return input_str, None\n",
    "        \n",
    "        notes, grade = process_string(ta_response)\n",
    "\n",
    "        return GradeLog(TA= self.ta_llm.model_identifier,\n",
    "                                Student= self.student_llm.model_identifier,\n",
    "                                Prompt=question,\n",
    "                                Response=student_response, \n",
    "                                Notes=notes, \n",
    "                                Grade=grade)\n",
    "\n",
    "\n",
    "eval_llm = Llm(model_identifier=\"gpt-4o\")\n",
    "eval_llm.prompt(\"poke, please reply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes from the CSV files\n",
    "rdf = pd.read_csv(\"evaluators/requirements_data/section_level_requirements.csv\")\n",
    "stdf = pd.read_csv(\"evaluators/requirements_data/section_types.csv\")\n",
    "rbsdf = pd.read_csv(\"evaluators/requirements_data/requirements_by_section_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"outputs/Llama_3_(Language_Model)/storm_gen_article_polished.txt\"\n",
    "article_name = path.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    section: str\n",
    "    section_type: str\n",
    "    subsection: str\n",
    "    paragraph_number: int\n",
    "    claim_number: int\n",
    "    text: str\n",
    "\n",
    "def identify_sections(text, level=1):\n",
    "    # Identify the sections\n",
    "    if level == 1:\n",
    "        sections= split = text.split(\"\\n# \")\n",
    "    elif level == 2:\n",
    "        sections = text.split(\"\\n## \")\n",
    "    else:\n",
    "        print(\"Invalid level\")\n",
    "\n",
    "    names = [section.split(\"\\n\")[0] for section in sections]\n",
    "    return names, sections\n",
    "\n",
    "def articulate_article(article):\n",
    "    # loop through sections\n",
    "    sentences = []\n",
    "    outline, sections = identify_sections(article)\n",
    "    for s in sections:\n",
    "        st = get_section_type(s)\n",
    "        suboutline,subsections = identify_sections(s, level=2)\n",
    "        for ss in subsections:\n",
    "            paragraphs = ss.split(\"\\n\")\n",
    "            for p in range(1,len(paragraphs)):\n",
    "                pa = paragraphs[p]\n",
    "                claims = pa.split(\". \")\n",
    "                for c in range(1,len(claims)):\n",
    "                    cl = claims[c]\n",
    "                    sentences.append(Sentence(outline[sections.index(s)],st,suboutline[subsections.index(ss)],p,c,cl))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def extract_response(response, default = \"body sections\"):\n",
    "        # Regular expression to find the first string between < and >\n",
    "    pattern = r'<(.*?)>'\n",
    "\n",
    "    # Search for the first match in the string\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    # If a match is found, return the matched string\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def get_section_type(section):\n",
    "    prompt = \"Section types are defined in \" + str(stdf.to_json()) + \"; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\n\" + section + \"\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "\n",
    "    return extract_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"r\") as file:\n",
    "        article = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = articulate_article(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_section_type(df.section[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exam_question(sentence):\n",
    "    prompt = \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\n\" +sentence.text + \"\\n\\nPlease return the question in the format: <question>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "    return extract_response(response)\n",
    "\n",
    "def make_question_guidelines(sentence):\n",
    "    prompt = \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\n\" +sentence.text + \"\\n\\nPlease return the question in the format: <question>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "    return extract_response(response)\n",
    "\n",
    "exam_guidelines = [\"evaluate according the to the requirements in the wikikpedia style guide\", \n",
    "                   \"provide a score between zero and one representing the probability this sentence meets the wikipedia guidelines according to a human wikipedia moderator\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [make_exam_question(sentence) for sentence in data]\n",
    "question_guidelines = [make_question_guidelines(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookup:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model_identifier = \"lookup\"\n",
    "        \n",
    "    def prompt_sequence(self,questions):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Exam(questions, question_guidelines, exam_guidelines, GradeLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [d.text for d in data]\n",
    "ex = LLMTest(Lookup(content), eval_llm, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g= ex.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_df = pd.DataFrame(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-numeric symbols from a string\n",
    "def remove_non_numeric(s):\n",
    "    return re.sub(r'[^\\d.]+', '', s)\n",
    "grade_df[\"Grade\"] = grade_df[\"Notes\"].apply(lambda x: remove_non_numeric(x.split(\"\\n\\n\")[0].split(\": \")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .8\n",
    "grade_df[\"Pass\"] = grade_df[\"Grade\"].apply(lambda x: float(x) > threshold)\n",
    "grade_df[\"Color\"] = grade_df[\"Pass\"].apply(lambda x: \"yellow\" if x else \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge grade df with df on index\n",
    "df = df.merge(grade_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Color==\"yellow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"outputs/Llama_3_(Language_Model)//graded_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[38]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
